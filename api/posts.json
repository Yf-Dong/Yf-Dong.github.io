{"total":5,"pageSize":10,"pageCount":1,"data":[{"title":"About Me","slug":"About-Me","date":"2023-04-09T12:38:52.000Z","updated":"2023-04-09T13:16:08.465Z","comments":true,"path":"api/articles/About-Me.json","excerpt":"<img src=\"/2023/04/09/About-Me/dyf_home.jpeg\" class=\"\" title=\"[cover]\">\n\n<center>Perhaps you would like to know me...</center>","keywords":null,"cover":"/2023/04/09/About-Me/dyf_home.jpeg","content":null,"text":" [Figure] Perhaps you would like to know me...My name is Dong Yifei. From 2017 to 2021, I obtained a Bachelor’s degree in Mechanical Design,","link":"","raw":null,"photos":[],"categories":[],"tags":[]},{"title":"ClusterFusion","slug":"ClusterFusion","date":"2023-04-06T13:06:41.000Z","updated":"2023-04-06T03:30:58.691Z","comments":true,"path":"api/articles/ClusterFusion.json","excerpt":"<img src=\"/2023/04/06/ClusterFusion/ClusterFusion_cover.jpg\" class=\"\" title=\"[cover]\">\n\n<p>As robotics technology advances, dense point cloud maps are increasingly in demand. However, dense reconstruction using a single unmanned aerial vehicle (UAV) suffers from limitations in flight speed and battery power, resulting in slow reconstruction and low coverage. Cluster UAV systems offer greater flexibility and wider coverage for map building. Existing methods of cluster UAVs face challenges with accurate relative positioning, scale drift, and high-speed dense point cloud map generation. </p>","keywords":null,"cover":"/2023/04/06/ClusterFusion/ClusterFusion_cover.jpg","content":null,"text":" [Figure]  ","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"Paper","slug":"Paper","count":3,"path":"api/tags/Paper.json"}]},{"title":"Indoor UAV Autonomous Detection and Tracking System","slug":"Indoor-UAV-Autonomous-Detection-and-Tracking-System","date":"2023-04-06T07:27:04.000Z","updated":"2023-04-07T15:47:57.198Z","comments":true,"path":"api/articles/Indoor-UAV-Autonomous-Detection-and-Tracking-System.json","excerpt":"<img src=\"/2023/04/06/Indoor-UAV-Autonomous-Detection-and-Tracking-System/uav_front.jpeg\" class=\"\" title=\"[cover]\">\n\n\n\n<p>The intelligent autonomous UAS design is carried out in the mission context of cooperative reconnaissance of multiple ground-moving targets in GPS denial environment by performing autonomous positioning, target identification, target tracking and other tasks to achieve accurate identification, continuous tracking and positioning of specific targets. </p>","keywords":null,"cover":"/2023/04/06/Indoor-UAV-Autonomous-Detection-and-Tracking-System/uav_front.jpeg","content":null,"text":" [Figure] A description of the software can be found here, but this content is in Chinese.The video can be found here (FIXME).","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"Project","slug":"Project","count":1,"path":"api/tags/Project.json"}]},{"title":"UWB-VO: Ultra-wideband Anchor Assisted Visual Odometry","slug":"UWB-VO-Ultra-wideband-Anchor-Assisted-Visual-Odometry","date":"2023-04-05T13:17:12.000Z","updated":"2023-04-05T15:02:50.692Z","comments":true,"path":"api/articles/UWB-VO-Ultra-wideband-Anchor-Assisted-Visual-Odometry.json","excerpt":"<img src=\"/2023/04/05/UWB-VO-Ultra-wideband-Anchor-Assisted-Visual-Odometry/UAV.bmp\" class=\"\" title=\"[cover]\">\t\n\n<p>​\tThis paper proposes a novel tightly coupled simultaneous localization and mapping (SLAM) method that combines monocular vision and ultra-wideband (UWB) technology. The proposed approach utilizes UWB distance information to restore the scale of monocular SLAM with an error rate of less than 1%. Our approach joint optimizes the distance residual between the camera and anchor, and the reprojection error of the map point, to improve SLAM positioning accuracy and mitigate visual SLAM degradation in challenging environments. A novel UWB anchor position optimization method is proposed, which enables the system to complete initialization within 2 seconds. The experimental results show that the proposed fusion scheme achieves significantly higher accuracy in system scale estimation and positioning compared to current state-of-the-art SLAM and visual-inertial odometry algorithms.</p>","keywords":null,"cover":"/2023/04/05/UWB-VO-Ultra-wideband-Anchor-Assisted-Visual-Odometry/UAV.bmp","content":null,"text":" [Figure]  ","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"Paper","slug":"Paper","count":3,"path":"api/tags/Paper.json"}]},{"title":"HybridFusion: LiDAR and Vision Cross-Source Point Cloud Fusion","slug":"HybridFusion-LiDAR-and-Vision-Cross-Source-Point-Cloud-Fusion","date":"2023-03-29T02:26:19.000Z","updated":"2023-04-06T03:25:32.726Z","comments":true,"path":"api/articles/HybridFusion-LiDAR-and-Vision-Cross-Source-Point-Cloud-Fusion.json","excerpt":"<img src=\"/2023/03/29/HybridFusion-LiDAR-and-Vision-Cross-Source-Point-Cloud-Fusion/example.jpg\" class=\"\" title=\"[cover]\">\n\n<p>Recently, cross-source point cloud registration from different sensors has become a significant research focus. However, traditional methods confront challenges due to the varying density and structure of cross-source point clouds. </p>","keywords":null,"cover":"/2023/03/29/HybridFusion-LiDAR-and-Vision-Cross-Source-Point-Cloud-Fusion/example.jpg","content":null,"text":" [Figure]  ","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"Paper","slug":"Paper","count":3,"path":"api/tags/Paper.json"}]}]}